{"meta":{"title":"Hexo","subtitle":"","description":"","author":"Siqin Dong","url":"https://siqindong.com","root":"/"},"pages":[{"title":"Books","date":"2022-02-01T20:21:57.236Z","updated":"2022-02-01T20:21:57.236Z","comments":false,"path":"books/index.html","permalink":"https://siqindong.com/books/index.html","excerpt":"","text":""},{"title":"About","date":"2022-02-01T20:21:57.236Z","updated":"2022-02-01T20:21:57.236Z","comments":false,"path":"about/index.html","permalink":"https://siqindong.com/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-02-01T20:21:57.237Z","updated":"2022-02-01T20:21:57.237Z","comments":false,"path":"categories/index.html","permalink":"https://siqindong.com/categories/index.html","excerpt":"","text":""},{"title":"Friends","date":"2022-02-01T20:21:57.237Z","updated":"2022-02-01T20:21:57.237Z","comments":true,"path":"links/index.html","permalink":"https://siqindong.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-02-01T20:21:57.237Z","updated":"2022-02-01T20:21:57.237Z","comments":false,"path":"repository/index.html","permalink":"https://siqindong.com/repository/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-02-01T20:21:57.237Z","updated":"2022-02-01T20:21:57.237Z","comments":false,"path":"tags/index.html","permalink":"https://siqindong.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Fourier Transform","slug":"Fourier-Transform","date":"2021-10-20T23:49:46.000Z","updated":"2022-02-01T20:21:57.235Z","comments":true,"path":"2021/10/20/Fourier-Transform/","link":"","permalink":"https://siqindong.com/2021/10/20/Fourier-Transform/","excerpt":"","text":"本文是对B站UP主Dr.CAN傅里叶变换系列视频的整理。 三角函数的正交性三角函数系：三角函数系是一个集合： \\begin{align*} \\lbrace 1,\\sin 1x,\\cos 1x,\\sin 2x,\\cos 2x\\ldots\\sin nx,\\cos nx\\ldots \\rbrace \\end{align*}即 \\begin{align*} 1, \\sin nx, \\cos nx \\quad n = 1,2,3 \\ldots \\end{align*}正交的概念：正交是一种二元关系，可以先从二维或三维世界里的垂直来理解，比如两个向量 $\\vec a$ 和 $\\vec b$，夹角 $\\phi$ 为 90 $^\\circ$，内积 $\\vec a\\cdot\\vec b = \\vert\\vec a\\vert\\vert\\vec b\\vert\\cos \\phi = 0$，所以如果两个向量正交，他们的内积等于0。把 $\\vec a$ 和 $\\vec b$ 在平面坐标系表达出来，比如 $\\vec a(2,1)$ 和 $\\vec b(-1,2)$，内积 $\\vec a\\cdot\\vec b = 2\\times(-1)+1\\times2 = 0$。 如果每个向量含有三个元素，比如 $\\vec a(1,2,5)$ 和 $\\vec b(1,2,-1)$，内积 $\\vec a\\cdot\\vec b = 1\\times1+2\\times2+5\\times\\ (-1) = 0$。 扩展一下，如果每个向量含有$n$个元素，$\\vec a(a_1,a_2,\\ldots a_n)$，$\\vec b(b_1,b_2,\\ldots b_n)$，则 \\begin{align*} \\vec a\\cdot\\vec b &= a_1b_1+a_2b_2+a_3b_3+\\ldots+a_nb_n \\\\ &= \\sum_{i=1}^na_ib_i \\\\ \\end{align*}继续扩展，如果 $a$ 是一个函数 $f(x)$，$b$ 是一个函数 $g(x)$，且在区间 [$x_0$,$x_1$] 上有定义，如果在区间 [$x_0$,$x_1$] 上一一对应地将 $f(x)$ 和 $g(x)$ 的值相乘，最后将所有乘积相加，因为 $f(x)$ 和 $g(x)$ 是连续函数，所以加和就是在 [$x_0$,$x_1$] 取积分，若积分等于0： \\begin{align*} \\int_{x_0}^{x_1}f(x)g(x) \\,{\\rm d}x = 0 \\end{align*}那么函数 $f(x)$ 和 $g(x)$ 在区间 [$x_0$,$x_1$] 正交。 三角函数的正交性是说从上面的三角函数系中，任取两个不同的函数相乘，然后在 $[-\\pi,\\pi]$ 积分，结果等于0： \\begin{align*} \\int_{-\\pi}^{\\pi}\\sin nx\\cos mx \\,{\\rm d}x = 0 \\quad n = m \\end{align*} \\begin{align*} \\int_{-\\pi}^{\\pi}\\sin nx\\cos mx \\,{\\rm d}x = 0 \\quad n \\neq m \\end{align*} \\begin{align*} \\int_{-\\pi}^{\\pi}\\sin nx\\sin mx \\,{\\rm d}x = 0 \\quad n \\neq m \\end{align*} \\begin{align*} \\int_{-\\pi}^{\\pi}\\cos nx\\cos mx \\,{\\rm d}x = 0 \\quad n \\neq m \\end{align*}若三角函数系中相同的两个函数相乘，然后在 $[-\\pi,\\pi]$ 积分，结果等于 $\\pi$ ： \\begin{align*} \\int_{-\\pi}^{\\pi}\\sin nx\\sin mx \\,{\\rm d}x = \\pi \\quad n = m \\end{align*} \\begin{align*} \\int_{-\\pi}^{\\pi}\\cos nx\\cos mx \\,{\\rm d}x = \\pi \\quad n = m \\end{align*}下面来证明一下，若$\\,n \\neq m$ ： 根据积化和差公式：$\\sin nx\\cos mx = \\frac{1}{2}[\\sin (n+m)x + \\sin (n-m)x]$ \\begin{align*} \\int_{-\\pi}^{\\pi}\\sin nx\\cos mx \\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}\\frac{1}{2} [\\sin (n+m)x + \\sin (n-m)x] \\,{\\rm d}x \\\\ &= \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\sin (n+m)x \\,{\\rm d}x}_0 + \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\sin (n-m)x \\,{\\rm d}x}_0 \\\\ &= 0 \\end{align*}根据积化和差公式：$\\sin nx\\sin mx = -\\frac{1}{2}[\\cos (n+m)x - \\cos (n-m)x]$ \\begin{align*} \\int_{-\\pi}^{\\pi}\\sin nx\\sin mx \\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}-\\frac{1}{2} [\\cos (n+m)x - \\cos (n-m)x] \\,{\\rm d}x \\\\ &= -\\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\cos (n+m)x \\,{\\rm d}x}_0 + \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\cos (n-m)x \\,{\\rm d}x}_0 \\\\ &= 0 \\end{align*}根据积化和差公式：$\\cos nx\\cos mx = \\frac{1}{2}[\\cos (n+m)x + \\cos (n-m)x]$ \\begin{align*} \\int_{-\\pi}^{\\pi}\\cos nx\\cos mx \\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}\\frac{1}{2} [\\cos (n+m)x + \\cos (n-m)x] \\,{\\rm d}x \\\\ &= \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\cos (n+m)x \\,{\\rm d}x}_0 + \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\cos (n-m)x \\,{\\rm d}x}_0 \\\\ &= 0 \\end{align*}若$\\,n = m$ ： 根据积化和差公式：$\\sin nx\\cos mx = \\frac{1}{2}[\\sin 2mx]$ \\begin{align*} \\int_{-\\pi}^{\\pi}\\sin nx\\cos mx \\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}\\frac{1}{2} [\\sin 2mx] \\,{\\rm d}x \\\\ &= \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\sin 2mx \\,{\\rm d}x}_0 \\\\ &= 0 \\end{align*}根据积化和差公式：$\\sin nx\\sin mx = -\\frac{1}{2}[\\cos 2mx - 1]$ \\begin{align*} \\int_{-\\pi}^{\\pi}\\sin nx\\sin mx \\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}-\\frac{1}{2}[\\cos 2mx - 1] \\,{\\rm d}x \\\\ &= -\\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\cos 2mx \\,{\\rm d}x}_0 + \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}1 \\,{\\rm d}x}_{2\\pi} \\\\ &= \\pi \\end{align*}根据积化和差公式：$\\cos nx\\cos mx = \\frac{1}{2}[\\cos 2mx + 1]$ \\begin{align*} \\int_{-\\pi}^{\\pi}\\cos nx\\cos mx \\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}\\frac{1}{2}[\\cos 2mx + 1] \\,{\\rm d}x \\\\ &= \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\cos 2mx \\,{\\rm d}x}_0 + \\frac{1}{2}\\underbrace{\\int_{-\\pi}^{\\pi}1 \\,{\\rm d}x}_{2\\pi} \\\\ &= \\pi \\end{align*} 周期为$\\,2\\pi\\,$的函数展开为傅里叶级数一个周期为$\\,2\\pi\\,$的函数，$T = 2\\pi$，$f(x) = f(x+2\\pi)$，可以展开为三角级数，也就是一系列三角函数的加和： \\begin{align*} f(x) &= \\sum_{n=0}^\\infty a_n \\cos nx + \\sum_{n=0}^\\infty b_n \\sin nx \\\\ &= a_0\\underbrace{\\cos 0x}_1 + \\sum_{n=1}^\\infty a_n \\cos nx + b_0\\underbrace{\\sin 0x}_0 + \\sum_{n=1}^\\infty b_n \\sin nx \\\\ &= a_0 + \\sum_{n=1}^\\infty a_n \\cos nx + \\sum_{n=1}^\\infty b_n \\sin nx \\tag{1}\\label{eq1} \\end{align*}第一步：求$\\,a_0\\,$对等式左右两边每一项在 $[-\\pi,\\pi]$ 积分： \\begin{align*} \\int_{-\\pi}^{\\pi}f(x)\\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}a_0\\,{\\rm d}x + \\int_{-\\pi}^{\\pi}\\sum_{n=1}^\\infty a_n \\cos nx\\,{\\rm d}x + \\int_{-\\pi}^{\\pi}\\sum_{n=1}^\\infty b_n \\sin nx\\,{\\rm d}x \\\\ &= \\int_{-\\pi}^{\\pi}a_0\\,{\\rm d}x + \\sum_{n=1}^\\infty a_n \\underbrace{\\int_{-\\pi}^{\\pi} \\cos nx\\,{\\rm d}x}_0 + \\sum_{n=1}^\\infty b_n \\underbrace{\\int_{-\\pi}^{\\pi} \\sin nx\\,{\\rm d}x}_0 \\\\ &= a_0\\int_{-\\pi}^{\\pi}1\\,{\\rm d}x \\\\ &= a_0x\\Big|_{-\\pi}^{\\pi} \\\\ &= 2\\pi a_0 \\\\ \\end{align*}求出$\\,\\eqref{eq1}\\,$中的$\\,a_0\\,$: \\begin{align*} a_0 = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi}f(x)\\,{\\rm d}x \\end{align*}如果我们把$\\,a_0\\,$放大2倍（但仍然用$\\,a_0\\,$表示）： \\begin{align*} a_0 &= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi}f(x)\\,{\\rm d}x \\end{align*}代入$\\,\\eqref{eq1}\\,$中，就需要缩小2倍： \\begin{align*} f(x) = \\frac{a_0}{2} + \\sum_{n=1}^\\infty a_n \\cos nx + \\sum_{n=1}^\\infty b_n \\sin nx \\tag{2}\\label{eq2} \\end{align*} Note $\\eqref{eq2}\\,$就是很多教科书上给出的公式了，将$\\,a_0\\,$换为$\\,\\frac{a_0}{2}\\,$似乎显得没有必要，其实这样做是为了和后面的$\\,a_n\\,$和$\\,b_n\\,$的格式统一，下面我们求出$\\,a_n\\,$和$\\,b_n\\,$就知道这样做的好处了。 第二步：求$\\,a_n\\,$对 $\\,\\eqref{eq2}\\,$ 左右两边每一项乘以$\\,\\cos mx\\,$，$m\\,$为定值，任取 $\\,0,1,2\\ldots\\,$，然后在 $[-\\pi,\\pi]$ 积分： \\begin{align*} \\int_{-\\pi}^{\\pi}f(x)\\cos mx\\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}\\frac{a_0}{2}\\cos mx\\,{\\rm d}x + \\int_{-\\pi}^{\\pi}\\sum_{n=1}^\\infty a_n \\cos nx\\cos mx\\,{\\rm d}x + \\int_{-\\pi}^{\\pi}\\sum_{n=1}^\\infty b_n \\sin nx\\cos mx\\,{\\rm d}x \\\\ &= \\frac{a_0}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\cos mx\\,{\\rm d}x}_0 + \\sum_{n=1}^\\infty a_n \\int_{-\\pi}^{\\pi} \\cos nx\\cos mx\\,{\\rm d}x + \\sum_{n=1}^\\infty b_n \\underbrace{\\int_{-\\pi}^{\\pi} \\sin nx\\cos mx\\,{\\rm d}x}_0 \\end{align*}由前面三角函数系的正交性可以知道式子右边第1项和第3项都为0，只剩下第2项： \\begin{align*} \\int_{-\\pi}^{\\pi}f(x)\\cos mx\\,{\\rm d}x &= \\sum_{n=1}^\\infty a_n \\int_{-\\pi}^{\\pi} \\cos nx\\cos mx\\,{\\rm d}x \\\\ \\end{align*}分析一下，第2项中存在$\\,n = m\\,$ 和 $\\,n \\neq m\\,$两种情况，由前面三角函数系的正交性可以知道所有$\\,n \\neq m\\,$的项都为0，所以只剩下$\\,n = m\\,$的那一项留下来了，上面的式子中的加和只剩下了一项： \\begin{align*} \\int_{-\\pi}^{\\pi}f(x)\\cos nx\\,{\\rm d}x &=a_n \\underbrace{\\int_{-\\pi}^{\\pi}\\cos nx\\cos nx\\,{\\rm d}x}_\\pi \\\\ \\end{align*}求出$\\,a_n\\,$: \\begin{align*} a_n &= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\cos nx\\,{\\rm d}x \\\\ \\end{align*}第三步：求$\\,b_n\\,$对 $\\,\\eqref{eq2}\\,$ 左右两边每一项乘以$\\,\\sin mx\\,$，$m\\,$为定值,任取 $1,2,3\\ldots\\,$，然后在 $[-\\pi,\\pi]$ 积分： \\begin{align*} \\int_{-\\pi}^{\\pi}f(x)\\sin mx\\,{\\rm d}x &= \\int_{-\\pi}^{\\pi}\\frac{a_0}{2}\\sin mx\\,{\\rm d}x + \\int_{-\\pi}^{\\pi}\\sum_{n=1}^\\infty a_n \\cos nx\\sin mx\\,{\\rm d}x + \\int_{-\\pi}^{\\pi}\\sum_{n=1}^\\infty b_n \\sin nx\\sin mx\\,{\\rm d}x \\\\ &= \\frac{a_0}{2}\\underbrace{\\int_{-\\pi}^{\\pi}\\sin mx\\,{\\rm d}x}_0 + \\sum_{n=1}^\\infty a_n \\underbrace{\\int_{-\\pi}^{\\pi} \\cos nx\\sin mx\\,{\\rm d}x}_0 + \\sum_{n=1}^\\infty b_n \\int_{-\\pi}^{\\pi} \\sin nx\\sin mx\\,{\\rm d}x \\end{align*}由前面三角函数系的正交性可以知道式子右边第1项和第2项都为0，只剩下第3项： \\begin{align*} \\int_{-\\pi}^{\\pi}f(x)\\sin mx\\,{\\rm d}x &= \\sum_{n=1}^\\infty b_n \\int_{-\\pi}^{\\pi} \\sin nx\\sin mx\\,{\\rm d}x \\\\ \\end{align*}同样，第3项中存在$\\,n = m\\,$ 和 $\\,n \\neq m\\,$两种情况，由前面三角函数系的正交性可以知道所有$\\,n \\neq m\\,$的项都为0，所以只剩下$\\,n = m\\,$的那一项留下来了，上面的式子中的加和只剩下了一项： \\begin{align*} \\int_{-\\pi}^{\\pi}f(x)\\sin nx\\,{\\rm d}x &=b_n \\underbrace{\\int_{-\\pi}^{\\pi}\\sin nx\\sin nx\\,{\\rm d}x}_\\pi \\\\ \\end{align*}求出$\\,b_n\\,$: \\begin{align*} b_n &= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\sin nx\\,{\\rm d}x \\\\ \\end{align*}好了，到此为止我们就求出了$\\,\\eqref{eq2}\\,$中的$\\,a_0\\,$，$a_n\\,$，$b_n\\,$，这样就得出了一个周期为$\\,2\\pi\\,$的函数的傅里叶级展开： $T = 2\\pi$，$f(x) = f(x+2\\pi)$ \\begin{align*} f(x) = \\frac{a_0}{2} + \\sum_{n=1}^\\infty [a_n \\cos nx + b_n \\sin nx] \\end{align*}其中： \\begin{align*} a_0 &= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi}f(x)\\,{\\rm d}x \\\\ a_n &= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\cos nx\\,{\\rm d}x \\\\ b_n &= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\sin nx\\,{\\rm d}x \\\\ \\end{align*}现在能发现为什么第一步求$\\,a_0\\,$时最后将$\\,a_0\\,$换为$\\,\\frac{a_0}{2}\\,$了。 周期为$\\,2L\\,$的函数展开为傅里叶级数下面讨论周期不是$\\,2\\pi\\,$而是其他任意值的函数的傅里叶级数展开","categories":[],"tags":[]},{"title":"在Matlab中读取并分析rosbag数据","slug":"在Matlab中读取并分析rosbag数据","date":"2021-03-17T01:09:58.000Z","updated":"2022-02-01T20:21:57.236Z","comments":true,"path":"2021/03/16/在Matlab中读取并分析rosbag数据/","link":"","permalink":"https://siqindong.com/2021/03/16/%E5%9C%A8Matlab%E4%B8%AD%E8%AF%BB%E5%8F%96%E5%B9%B6%E5%88%86%E6%9E%90rosbag%E6%95%B0%E6%8D%AE/","excerpt":"","text":"我最近在用Sawyer机器人做实验，用rosbag记录了机器人状态数据，后续要进行可视化和分析，虽然ROS内置rqt_plot有绘图功能，但是rqt_plot的功能比较简单，数据量大的时候查看起来也比较麻烦，还是导入进Matlab分析更好一些。 Matlab可以直接读取rosbag数据，对于ROS自带的topic信息，比如 /joint_states 和 /cmd_vel，用Matlab读取很方便。但是对于使用了自定义的message类型的topic，需要重新编译存放msg文件的ROS package，这一步骤在Matlab中使用rosgenmsg命令完成，借助Matlab的java接口，Matlab就能识别自定义的message了。 我的Matlab版本是R2021a，需要注意的是在R2020b版本中Matlab对ROS功能的支持做了很大改动，rosgenmsg 直接包含在了toolbox中（不需要额外的support package)，网上有的教程写需要通过roboticsAddons命令安装插件，应该用的是R2020b之前的Matlab版本。以后可以直接看这份官方文档：ROS Custom Message Support，具体操作跟着文档走即可，这里主要是记录一下需要注意的地方。 更新Cmake版本Cmake版本要确保在3.15.5或以上，在Matlab官方文档ROS System Requirements也可以查到。 检查你目前的Cmake版本：1cmake --version 如果版本低于3.15.5则需要更新，最常见的想法可能是直接在Linux终端用sudo apt-get install cmake安装，但是安装完发现是低于要求的版本，比如我用的Ubuntu 16.04，只能得到3.5.1，用sudo apt-get install cmake安装不了最新版本的原因是操作系统本身的repositories没有更新，16.04是Ubuntu的长期支持（LTS）版本，要在5年内保持稳定，一般只会进行关键或者安全方面的更新，不会经常主动更新最新版本的packages，通常每6个月才会更新一次。 推荐安装方法如下： 卸载Ubuntu提供的默认版本：1sudo apt remove --purge --auto-remove cmake 或者1sudo apt purge --auto-remove cmake 去CMake官网下载界面，查看Cmake版本号，确定要安装的版本（修改version和build变量到需要的版本号，下例：3.19.1）：12version=3.19build=1 先新建一个文件夹temp来存放cmake源码包，然后获取cmake源码包：123mkdir ~/tempcd ~/tempwget https://cmake.org/files/v$version/cmake-$version.$build.tar.gz 解压源码包:1tar -xzvf cmake-$version.$build.tar.gz 进入解压后的cmake目录:1cd cmake-$version.$build/ 安装cmake： 123./bootstrapmake -j$(nproc)sudo make install Note: 因为我的是多核CPU，这里使用了make -j$(nproc)命令进行并行编译，不过直接用make出问题的概率小一点，nproc资料可以看这里 make install命令需要root privilege 最后，打开一个新终端，检查Cmake版本： 1cmake --version Note: cmake --version要在新终端才有用，因为用上面的方法Cmake会默认安装在/usr/local/bin/，而如果用开始提到的sudo apt-get install cmake安装，默认安装路径会是/usr/bin/，原因可以看StackExchange上这个回答：/usr/local/bin is for normal user programs not managed by the distribution package manager, e.g. locally compiled packages. You should not install them into /usr/bin because future distribution upgrades may modify or delete them without warning. cmake --version运行的结果: 123cmake version 3.19.1CMake suite maintained and supported by Kitware (kitware.com/cmake). 确认Cmake路径更新好Cmake版本后，为了确保Cmake在Matlab中可用，需要保证Cmake在Matlab的环境变量中： 首先在Linux终端中查看Cmake路径，上面已经提过Cmake会默认安装在/usr/local/bin/，可以在终端查看：1which cmake 运行结果：1/usr/local/bin/cmake 然后打开Mtalab，命令行运行：1!which cmake 运行结果应该和前面Linux终端查到的路径一致：1/usr/local/bin/cmake 最后也在Matlab查看一下Cmake版本：1!cmake --version 运行结果应和前面Linux终端查到版本号一致：123cmake version 3.19.1CMake suite maintained and supported by Kitware (kitware.com/cmake). 编译自定义message这里用Sawyer提供的ROS packageintera_core_msgs举例，自定义的message文件就在msg里面。我先建了一个文件夹custom_msgs，然后在里面再建个文件夹sawyer_custom_msg_matlab包含这个package，以后有其他package也可以放到这个文件夹里面。intera_core_msgspackage我是直接从Github上复制进来的，里面已经生成且配置好CmakeLists.txt和package.xml文件了，如果你想自己create ROS package也可以，注意别忘了修改这两个文件。后面的matlab_msg_gen_ros1文件夹就是用Matlab编译生成的，下面会演示。 Note 因为只要用到msg，我复制intera_core_msgspackage时没有复制srv，需要的时候再加进去。 目前不支持ROS actions，在自定义message生成期间将被忽略。 接下来就可以在Matlab里面用rosgenmsg命令编译了： 声明自定义message所在ROS package的路径：1sawyer_folder = &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab&#x27; 运行结果：123sawyer_folder = &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab&#x27; Call rosgenmsg:1rosgenmsg(sawyer_folder) 运行结果：12345678910111213141516171819202122Identifying message files in folder &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab&#x27;..Done.Validating message files in folder &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab&#x27;..Done.[1/1] Generating MATLAB interfaces for custom message packages... Done.Running catkin build in folder &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab/matlab_msg_gen_ros1/glnxa64&#x27;.Build in progress. This may take several minutes...Build succeeded.build log To use the custom messages, follow these steps: 1. Add the custom message folder to the MATLAB path by executing: addpath(&#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab/matlab_msg_gen_ros1/glnxa64/install/m&#x27;)savepath 2. Refresh all message class definitions, which requires clearing the workspace, by executing: clear classesrehash toolboxcache 3. Verify that you can use the custom messages. Enter &quot;rosmsg list&quot; and ensure that the output contains the generated custom message types. 然后按照上面的提示说明加入环境变量，清理一下工作空间就可以了，逐条运行：1234addpath(&#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab/matlab_msg_gen_ros1/glnxa64/install/m&#x27;)savepathclear classesrehash toolboxcache Note执行savepath时如果提示无法修改Matlab路径文件的话，有两种解决方法： 第一种：在当前Matlab命令窗口修改权限：查看pathdef路径:1which pathdef 运行结果：1/usr/local/MATLAB/R2021a/toolbox/local/pathdef.m 修改成所有user都有读和写的权限：1sudo chmod 666 /usr/local/MATLAB/R2018a/toolbox/local/pathdef.m 第二种：在Linux终端用sudo matlab重新打开Matlab 最后用查看自定义的message types是否被成功生成：1rosmsg list可以发现intera_core_msgspackage下msg文件夹里面的所有自定义message都已被成功生成：1234567891011121314151617181920212223242526272829303132333435intera_core_msgs/AnalogIOState intera_core_msgs/AnalogIOStates intera_core_msgs/AnalogOutputCommand intera_core_msgs/CameraControl intera_core_msgs/CameraSettings intera_core_msgs/CollisionAvoidanceState intera_core_msgs/CollisionDetectionState intera_core_msgs/DigitalIOState intera_core_msgs/DigitalIOStates intera_core_msgs/DigitalOutputCommand intera_core_msgs/EndpointNamesArray intera_core_msgs/EndpointState intera_core_msgs/EndpointStates intera_core_msgs/HeadPanCommand intera_core_msgs/HeadState intera_core_msgs/HomingCommand intera_core_msgs/HomingState intera_core_msgs/IOComponentCommand intera_core_msgs/IOComponentConfiguration intera_core_msgs/IOComponentStatus intera_core_msgs/IODataStatus intera_core_msgs/IODeviceConfiguration intera_core_msgs/IODeviceStatus intera_core_msgs/IONodeConfiguration intera_core_msgs/IONodeStatus intera_core_msgs/IOStatus intera_core_msgs/InteractionControlCommand intera_core_msgs/InteractionControlState intera_core_msgs/JointCommand intera_core_msgs/JointLimits intera_core_msgs/NavigatorState intera_core_msgs/NavigatorStates intera_core_msgs/RobotAssemblyState intera_core_msgs/SEAJointState intera_core_msgs/URDFConfiguration rosbag数据处理接下来以intera_core_msgs/EndpointState这个自定义message为例，用Matlab处理rosbag记录的机器人数据。 bag文件读取: 1bag = rosbag(&#x27;/home/siqin/catkin_ws/sawyer_end_point_state.bag&#x27;); 运行后点击Workspace的bag变量，得到如下图所示信息： 选择Topic 点击上图中的AvaliableTopics查看记录有哪些Topic，我这里只记录了一个/robot/limb/right/endpoint_state，如下图所示： 查看topic之后利用select函数选择需要处理的Topic名称: 1state_select = select(bag, &#x27;Time&#x27;,[bag.StartTime bag.EndTime], &#x27;Topic&#x27;, &#x27;/robot/limb/right/endpoint_state&#x27;); 从上图还可以注意到MessageType列中的intera_core_msgs/EndpointState正是我们之前自定义的message。 利用readMessages函数读取select函数选择的数据 1stateMsgs = readMessages(state_select); 运行后点击Workspace的stateMsgs变量，得到如下图所示信息： 可以看到stateMsgs是一个1408x1的array，包含了记录的1408条message数据，点击第一条看看： 上图显示的是定义intera_core_msgs/EndpointState的数据结构，我们可以先在Linux终端看一下具体内容： 1rosmsg show intera_core_msgs/EndpointState 运行结果： 可以看到intera_core_msgs/EndpointState使用了std_msgs和geometry_msgs两种ROS自带的message来定义，主要包含了机器人TCP的以下信息： pose（位姿）： positon 位置： x y z坐标 orinentation 姿态： 用四元数表示 twist（旋量，6维向量）： linear： 沿轴的线速度，3维向量 angular： 绕轴的角速度 3维向量 wrench（旋量，6维向量）： force： 力，3维向量 torque： 力矩，3维向量 Note： 四元数和旋量理论后面有空单独写几篇讨论 以pose中的position为例：点击上图stateMsgs中的1x1 Pose，得到如下图所示信息： 可以看到Pose包含了positon和orinentation信息，点开positon： positon包含了机器人TCP的x y z坐标，与在终端查看的结果一致，点开X的信息： Note： 图中每个tab的名称代表的是当前数据的读取方式，比如stateMsg{1,1}.Pose.Position.X表示的是X的读取方式。 接下来可视化数据，建立一个三维数组，将所有position数据读入数组： 123456TCP_position=zeros(1408,3);for i=1:1408 TCP_position(i,1)=stateMsgs&#123;i,1&#125;.Pose.Position.X; TCP_position(i,2)=stateMsgs&#123;i,1&#125;.Pose.Position.Y; TCP_position(i,3)=stateMsgs&#123;i,1&#125;.Pose.Position.Z;end 绘制轨迹图： 123456789101112figure;for i=1:3 plot((1:1408),TCP_position(:,i),&#x27;LineWidth&#x27;,1.5); hold on;endxlabel(&#x27;seq&#x27;);title(&#x27;Position of End Point&#x27;);grid on;legend(&#x27;Position.X&#x27;,&#x27;Position.Y&#x27;,&#x27;Position.Z&#x27;); figure;comet3(position(:,1),position(:,2),position(:,3),0.5); 完整Matlab代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263clear all;close all;clc;bag = rosbag(&#x27;/home/siqin/catkin_ws/sawyer_end_point_state.bag&#x27;);state_select = select(bag, &#x27;Time&#x27;,[bag.StartTime bag.EndTime], &#x27;Topic&#x27;, &#x27;/robot/limb/right/endpoint_state&#x27;);% &#123;&#x27;intera_core_msgs/EndpointState&#x27;&#125;stateMsgs = readMessages(state_select);% rosmsg show intera_core_msgs/EndpointState% std_msgs/Header header% uint32 seq% time stamp% string frame_id% geometry_msgs/Pose pose% geometry_msgs/Point position% float64 x% float64 y% float64 z% geometry_msgs/Quaternion orientation% float64 x% float64 y% float64 z% float64 w% geometry_msgs/Twist twist% geometry_msgs/Vector3 linear% float64 x% float64 y% float64 z% geometry_msgs/Vector3 angular% float64 x% float64 y% float64 z% geometry_msgs/Wrench wrench% geometry_msgs/Vector3 force% float64 x% float64 y% float64 z% geometry_msgs/Vector3 torque% float64 x% float64 y% float64 z% bool validTCP_position=zeros(1408,3);for i=1:1408 TCP_position(i,1)=stateMsgs&#123;i,1&#125;.Pose.Position.X; TCP_position(i,2)=stateMsgs&#123;i,1&#125;.Pose.Position.Y; TCP_position(i,3)=stateMsgs&#123;i,1&#125;.Pose.Position.Z;endfigure;for i=1:3 plot((1:1408),TCP_position(:,i),&#x27;LineWidth&#x27;,1.5); hold on;endxlabel(&#x27;seq&#x27;);title(&#x27;Position of End Point&#x27;);grid on;legend(&#x27;Position.X&#x27;,&#x27;Position.Y&#x27;,&#x27;Position.Z&#x27;);figure;comet3(TCP_position(:,1),TCP_position(:,2),TCP_position(:,3),0.5); References[1] https://www.mathworks.com/help/ros/ug/ros-custom-message-support.html[2] https://askubuntu.com/questions/355565/how-do-i-install-the-latest-version-of-cmake-from-the-command-line[3] https://www.mathworks.com/matlabcentral/answers/623103-matlab-2020b-rosgenmsg-can-t-find-cmake[4] https://blog.csdn.net/yaked/article/details/97682872[5] https://blog.csdn.net/weixin_40712763/article/details/78909608[6] https://blog.csdn.net/u012424737/article/details/106766307","categories":[{"name":"Tutorial","slug":"Tutorial","permalink":"https://siqindong.com/categories/Tutorial/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://siqindong.com/tags/ROS/"},{"name":"Matlab","slug":"Matlab","permalink":"https://siqindong.com/tags/Matlab/"}]},{"title":"Read and analyze rosbag data in Matlab","slug":"Read-and-analyze-rosbag-data-in-Matlab","date":"2021-03-17T01:09:58.000Z","updated":"2022-02-01T20:21:57.236Z","comments":true,"path":"2021/03/16/Read-and-analyze-rosbag-data-in-Matlab/","link":"","permalink":"https://siqindong.com/2021/03/16/Read-and-analyze-rosbag-data-in-Matlab/","excerpt":"","text":"I was experimenting with the Sawyer robot and using rosbag to record the robot status data, which need to be visualized and analyzed later. Usually the built-in rqt_plot in ROS can be used to have a overview of the data. However, rqt_plot is relatively simple and not handy when the data size is large. So it is better to import data into Matlab for analysis. Matlab can directly read rosbag data. For topics common used in ROS, such as /joint_states and /cmd_vel, it is very convenient to use Matlab to read. However, topics that use a custom message type, you will need to recompile the ROS package that stores the msg file. This step can be completed using rosgenmsg command in Matlab, with the help of Matlab’s java interface, Matlab will be able to recognize the custom message. My Matlab version is R2021a when I was writing this blog. Note that in the R2020b version, Matlab’s support for ROS has been greatly changed. rosgenmsg is directly included in the toolbox (no additional support package is required). You may found some online tutorials use the roboticsAddons command to install rosgenmsg plugin, maybe the Matlab version they used is before R2020b. You may read this official document directly in the future: ROS Custom Message Support, just follow the documentation step by step, purpose of this blog is mainly to record some tricky problems that need attention when the first time you are dealing with it. Update Cmake versionMake sure your Cmake version is 3.15.5 or later, this requirement can be found in official Matlab document ROS System Requirements. Check your current Cmake version:1cmake --versionIf the version is older than 3.15.5, it needs to be updated. The most common way is directly with sudo apt-get install cmake in the Linux terminal, but after installation, you may found the version is still older than the required version, such as Ubuntu 16.04 I was using, I can only get 3.5.1. The reason why I can not install the latest version with sudo apt-get install cmake is that the repositories of the operating system itself have not been updated. 16.04 is the long-term support (LTS) version of Ubuntu, and it must be stable within 5 years. Generally, only critical or security updates are performed, and the latest versions of packages are not updated frequently, usually every 6 months. Recommended installation method: Uninstall the default version provided by Ubuntu:1sudo apt remove --purge --auto-remove cmake Or1sudo apt purge --auto-remove cmake Go to CMake official website download interface, check the Cmake version number, and determine the version to be installed (modify the version and build variables to the required version number, following example: 3.19.1):12version=3.19build=1 Create a temp folder to store the cmake source package, and download the cmake source package:123mkdir ~/tempcd ~/tempwget https://cmake.org/files/v$version/cmake-$version.$build.tar.gz Unzip the source package:1tar -xzvf cmake-$version.$build.tar.gz Enter the unzipped cmake directory:1cd cmake-$version.$build/ Install cmake： 123./bootstrapmake -j$(nproc)sudo make install Note: Because I have a multi-core CPU, the make -j$(nproc) command is used here for parallel compilation, just using make is also ok, the nproc information can be found here make install command requires root privilege Finally, open a new terminal and check the Cmake version: 1cmake --version Note: cmake --version is only valid in a new terminal, because for the method used above, Cmake will be installed in /usr/local/bin/ by default, and if you use the sudo apt-get install cmake mentioned at the beginning, the default installation path will be /usr/bin/, the reason can be seen This answer on StackExchange：/usr/local/bin is for normal user programs not managed by the distribution package manager, e.g. locally compiled packages. You should not install them into /usr/bin because future distribution upgrades may modify or delete them without warning. Result after running cmake --version: 123cmake version 3.19.1CMake suite maintained and supported by Kitware (kitware.com/cmake). Confirm Cmake pathAfter updating the Cmake version, in order to ensure Cmake is available in Matlab, you need to check Cmake is on the environment variables of Matlab: First, check the Cmake path in the Linux terminal. As mentioned above, Cmake will be installed by default in /usr/local/bin/, which can be check in the terminal:1which cmake Result：1/usr/local/bin/cmake Then open Mtalab and run the command:1!which cmake The result should be consistent with the path found in the Linux terminal earlier:1/usr/local/bin/cmake Finally, check the Cmake version in Matlab:1!cmake --version The running result should be consistent with the version number found in the Linux terminal earlier:123cmake version 3.19.1CMake suite maintained and supported by Kitware (kitware.com/cmake). Compile the custom messageHere I use the ROS package intera_core_msgs provided by Sawyer as an example. The custom message file is in msg folder. I first create a folder custom_msgs, and create a folder sawyer_custom_msg_matlab inside it to contain the package, other packages also can be placed in this folder in the future. I copied the intera_core_msgs package directly from Github, which has already generated and configured CmakeLists.txt and package.xml file, if you wish to create a ROS package yourself, don’t forget to modify these two files. The following matlab_msg_gen_ros1 folder is generated by compiling with Matlab, which will be demonstrated below. Note Only msg is used here, I did not copy srv when coping the intera_core_msgs package, I can add it when it is needed. ROS actions are not supported currently, it will be ignored during custom message generation. Next, you can use the rosgenmsg command to compile in Matlab: Declare the path of the ROS package where the custom message is located:1sawyer_folder = &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab&#x27; Result:123sawyer_folder = &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab&#x27; Call rosgenmsg:1rosgenmsg(sawyer_folder) Result：12345678910111213141516171819202122Identifying message files in folder &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab&#x27;..Done.Validating message files in folder &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab&#x27;..Done.[1/1] Generating MATLAB interfaces for custom message packages... Done.Running catkin build in folder &#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab/matlab_msg_gen_ros1/glnxa64&#x27;.Build in progress. This may take several minutes...Build succeeded.build log To use the custom messages, follow these steps: 1. Add the custom message folder to the MATLAB path by executing: addpath(&#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab/matlab_msg_gen_ros1/glnxa64/install/m&#x27;)savepath 2. Refresh all message class definitions, which requires clearing the workspace, by executing: clear classesrehash toolboxcache 3. Verify that you can use the custom messages. Enter &quot;rosmsg list&quot; and ensure that the output contains the generated custom message types. Then follow the instructions above to add environment variables, clear the workspace, and run the following commands one by one:1234addpath(&#x27;/home/siqin/Documents/MATLAB/custom_msgs/sawyer_custom_msg_matlab/matlab_msg_gen_ros1/glnxa64/install/m&#x27;)savepathclear classesrehash toolboxcache NoteWhen executing savepath, if you are prompted that the Matlab path file cannot be modified, there are two solutions: Method 1: Modify permissions in the current Matlab command window:Check pathdef path:1which pathdef Result：1/usr/local/MATLAB/R2021a/toolbox/local/pathdef.m Modify it so that all users have read and write permissions:1sudo chmod 666 /usr/local/MATLAB/R2018a/toolbox/local/pathdef.m Method 2: Reopen Matlab with sudo matlab in the Linux terminal Finally, use to see if the custom message types are successfully generated：1rosmsg listWe can see that all custom messages in the msg folder under the intera_core_msgs package have been successfully generated:1234567891011121314151617181920212223242526272829303132333435intera_core_msgs/AnalogIOState intera_core_msgs/AnalogIOStates intera_core_msgs/AnalogOutputCommand intera_core_msgs/CameraControl intera_core_msgs/CameraSettings intera_core_msgs/CollisionAvoidanceState intera_core_msgs/CollisionDetectionState intera_core_msgs/DigitalIOState intera_core_msgs/DigitalIOStates intera_core_msgs/DigitalOutputCommand intera_core_msgs/EndpointNamesArray intera_core_msgs/EndpointState intera_core_msgs/EndpointStates intera_core_msgs/HeadPanCommand intera_core_msgs/HeadState intera_core_msgs/HomingCommand intera_core_msgs/HomingState intera_core_msgs/IOComponentCommand intera_core_msgs/IOComponentConfiguration intera_core_msgs/IOComponentStatus intera_core_msgs/IODataStatus intera_core_msgs/IODeviceConfiguration intera_core_msgs/IODeviceStatus intera_core_msgs/IONodeConfiguration intera_core_msgs/IONodeStatus intera_core_msgs/IOStatus intera_core_msgs/InteractionControlCommand intera_core_msgs/InteractionControlState intera_core_msgs/JointCommand intera_core_msgs/JointLimits intera_core_msgs/NavigatorState intera_core_msgs/NavigatorStates intera_core_msgs/RobotAssemblyState intera_core_msgs/SEAJointState intera_core_msgs/URDFConfiguration Rosbag data processingNext, I will take the custom message intera_core_msgs/EndpointState as an example, and use Matlab to process the robot data recorded by rosbag. Read bag file: 1bag = rosbag(&#x27;/home/siqin/catkin_ws/sawyer_end_point_state.bag&#x27;); After running, click the bag variable in the Workspace to get the information shown in the following figure: Select Topic Click AvailableTopics in the above picture to review what topics are recorded. I only recorded one topic /robot/limb/right/endpoint_state here, as shown in the following figure: After reviewing the topic, use the select function to select the topic name to be processed: 1state_select = select(bag, &#x27;Time&#x27;,[bag.StartTime bag.EndTime], &#x27;Topic&#x27;, &#x27;/robot/limb/right/endpoint_state&#x27;); In the figure as shown above, you may also notice that the intera_core_msgs/EndpointState in the MessageType column is the message we customized before. Use the readMessages function to read the data selected by the select function 1stateMsgs = readMessages(state_select); After running, click the stateMsgs variable in the Workspace to get the information shown in the following figure: You can see that stateMsgs is a 1408x1 array, which contains 1408 message data records, click on the first one: The figure above shows the data structure that defines intera_core_msgs/EndpointState. We can first look at the content in the Linux terminal: 1rosmsg show intera_core_msgs/EndpointState Result： We can see that intera_core_msgs/EndpointState uses two ROS built-in messages std_msgs and geometry_msgs, intera_core_msgs/EndpointState mainly contains the following information of the robot TCP: pose: positon: x y z coordinates orinentation: represented by quaternions twist (a screw, 6-dimensional vector): linear: linear velocity along an axis, a 3-dimensional vector angular: angular velocity about an axis, a 3-dimensional vector wrench (a screw, 6-dimensional vector): force: a 3-dimensional vector torque: a 3-dimensional vector Note: Quaternion and Screw theory will be discussed separately later if I have time. Take the position in pose as an example: click 1x1 Pose in the stateMsgs shown in the above figure, get the information shown in the figure below: You can see that Pose contains positon and orinentation information, click on positon: The positon contains the x y z coordinates of the robot TCP, which are consistent with the results viewed in the terminal. Click on the information of X: Note： The name of each tab in the figure represents the method of accessing the current data. For example, stateMsg{1,1}.Pose.Position.X represents the method of accessing X. Next, to visualize the data, we can create a three-dimensional array, and put all the position data into the array: 123456TCP_position=zeros(1408,3);for i=1:1408 TCP_position(i,1)=stateMsgs&#123;i,1&#125;.Pose.Position.X; TCP_position(i,2)=stateMsgs&#123;i,1&#125;.Pose.Position.Y; TCP_position(i,3)=stateMsgs&#123;i,1&#125;.Pose.Position.Z;end Draw a trajectory graph: 123456789101112figure;for i=1:3 plot((1:1408),TCP_position(:,i),&#x27;LineWidth&#x27;,1.5); hold on;endxlabel(&#x27;seq&#x27;);title(&#x27;Position of End Point&#x27;);grid on;legend(&#x27;Position.X&#x27;,&#x27;Position.Y&#x27;,&#x27;Position.Z&#x27;); figure;comet3(position(:,1),position(:,2),position(:,3),0.5); Complete Matlab code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263clear all;close all;clc;bag = rosbag(&#x27;/home/siqin/catkin_ws/sawyer_end_point_state.bag&#x27;);state_select = select(bag, &#x27;Time&#x27;,[bag.StartTime bag.EndTime], &#x27;Topic&#x27;, &#x27;/robot/limb/right/endpoint_state&#x27;);% &#123;&#x27;intera_core_msgs/EndpointState&#x27;&#125;stateMsgs = readMessages(state_select);% rosmsg show intera_core_msgs/EndpointState% std_msgs/Header header% uint32 seq% time stamp% string frame_id% geometry_msgs/Pose pose% geometry_msgs/Point position% float64 x% float64 y% float64 z% geometry_msgs/Quaternion orientation% float64 x% float64 y% float64 z% float64 w% geometry_msgs/Twist twist% geometry_msgs/Vector3 linear% float64 x% float64 y% float64 z% geometry_msgs/Vector3 angular% float64 x% float64 y% float64 z% geometry_msgs/Wrench wrench% geometry_msgs/Vector3 force% float64 x% float64 y% float64 z% geometry_msgs/Vector3 torque% float64 x% float64 y% float64 z% bool validTCP_position=zeros(1408,3);for i=1:1408 TCP_position(i,1)=stateMsgs&#123;i,1&#125;.Pose.Position.X; TCP_position(i,2)=stateMsgs&#123;i,1&#125;.Pose.Position.Y; TCP_position(i,3)=stateMsgs&#123;i,1&#125;.Pose.Position.Z;endfigure;for i=1:3 plot((1:1408),TCP_position(:,i),&#x27;LineWidth&#x27;,1.5); hold on;endxlabel(&#x27;seq&#x27;);title(&#x27;Position of End Point&#x27;);grid on;legend(&#x27;Position.X&#x27;,&#x27;Position.Y&#x27;,&#x27;Position.Z&#x27;);figure;comet3(TCP_position(:,1),TCP_position(:,2),TCP_position(:,3),0.5); References[1] https://www.mathworks.com/help/ros/ug/ros-custom-message-support.html[2] https://askubuntu.com/questions/355565/how-do-i-install-the-latest-version-of-cmake-from-the-command-line[3] https://www.mathworks.com/matlabcentral/answers/623103-matlab-2020b-rosgenmsg-can-t-find-cmake[4] https://blog.csdn.net/yaked/article/details/97682872[5] https://blog.csdn.net/weixin_40712763/article/details/78909608[6] https://blog.csdn.net/u012424737/article/details/106766307","categories":[{"name":"Tutorial","slug":"Tutorial","permalink":"https://siqindong.com/categories/Tutorial/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://siqindong.com/tags/ROS/"},{"name":"Matlab","slug":"Matlab","permalink":"https://siqindong.com/tags/Matlab/"}]},{"title":"Kalman Filter","slug":"Kalman-Filter","date":"2020-12-17T01:28:37.000Z","updated":"2022-02-01T20:21:57.235Z","comments":true,"path":"2020/12/16/Kalman-Filter/","link":"","permalink":"https://siqindong.com/2020/12/16/Kalman-Filter/","excerpt":"","text":"","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://siqindong.com/categories/Algorithm/"}],"tags":[]},{"title":"Gaussian Processes","slug":"Gaussian-Processes","date":"2020-11-14T05:11:14.000Z","updated":"2022-02-01T20:21:57.235Z","comments":true,"path":"2020/11/14/Gaussian-Processes/","link":"","permalink":"https://siqindong.com/2020/11/14/Gaussian-Processes/","excerpt":"","text":"IntroductionIn supervised learning, we often use parametric models $p(\\mathbf{y} \\lvert \\mathbf{X},\\boldsymbol\\theta)$ to explain data and infer optimal values of parameter $\\boldsymbol\\theta$ via maximum likelihood or maximum a posteriori estimation. If needed we can also infer a full posterior distribution $p(\\boldsymbol\\theta \\lvert \\mathbf{X},\\mathbf{y})$ instead of a point estimate $\\boldsymbol{\\hat\\theta}$. With increasing data complexity, models with a higher number of parameters are usually needed to explain data reasonably well. Methods that use models with a fixed number of parameters are called parametric methods. In non-parametric methods, on the other hand, the number of parameters depend on the dataset size. For example, in Nadaraya-Watson kernel regression, a weight $w_i$ is assigned to each observed target $y_i$ and for predicting the target value at a new point $\\mathbf{x}$ a weighted average is computed: \\begin{align*} f(\\mathbf{x}) &= \\sum_{i=1}^{N}w_i(\\mathbf{x})y_i \\\\ w_i(\\mathbf{x}) &= \\frac{\\kappa(\\mathbf{x}, \\mathbf{x}_{i})}{\\sum_{i'=1}^{N}\\kappa(\\mathbf{x}, \\mathbf{x}_{i'})} \\end{align*}Observations that are closer to $\\mathbf{x}$ have a higher weight than observations that are further away. Weights are computed from $\\mathbf{x}$ and observed $\\mathbf{x}_i$ with a kernel $\\kappa$. A special case is k-nearest neighbors (KNN) where the $k$ closest observations have a weight $1/k$, and all others have weight $0$. Non-parametric methods often need to process all training data for prediction and are therefore slower at inference time than parametric methods. On the other hand, training is usually faster as non-parametric models only need to remember training data. Another example of non-parametric methods are Gaussian processes (GPs). Instead of inferring a distribution over the parameters of a parametric function Gaussian processes can be used to infer a distribution over functions directly. A Gaussian process defines a prior over functions. After having observed some function values it can be converted into a posterior over functions. Inference of continuous function values in this context is known as GP regression but GPs can also be used for classification. Univariate Gaussian distributionThe probability density function of a univariate Gaussian distribution: \\begin{align*} p(\\mathbf{x}) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp (-\\frac{(\\mathbf{x}-\\boldsymbol{\\mu})^2}{2\\sigma^2}) \\tag{1}\\label{eq1} \\end{align*}$\\boldsymbol{\\mu}$ is the mean or expectation of the distribution (and also its median and mode), while the parameter $\\sigma$ is its standard deviation.The variance of the distribution is $\\sigma^2$. Multivariate Gaussian distributionThe multivariate Gaussian distribution is a generalization of the univariate Gaussian distribution to higher dimensions. Assuming that the dimensions are independent of each other: \\begin{align*} p(\\mathbf{x}_{1}, \\mathbf{x}_{2}, ..., \\mathbf{x}_{N}) &= \\prod_{i=1}^{N}p(\\mathbf{x}_{i}) \\\\ &= \\frac{1}{(2\\pi)^{\\frac{N}{2}}\\sigma_1\\sigma_2...\\sigma_N}\\exp \\left(-\\frac{1}{2}\\left [\\frac{(\\mathbf{x}_{1}-\\boldsymbol{\\mu}_{1})^2}{\\sigma_1^2} + \\frac{(\\mathbf{x}_{2}-\\boldsymbol{\\mu}_{2})^2}{\\sigma_2^2} + ... + \\frac{(\\mathbf{x}_{N}-\\boldsymbol{\\mu}_{N})^2}{\\sigma_N^2}\\right]\\right) \\tag{2}\\label{eq2} \\end{align*}Equation $(2)$ in matrix form: \\begin{align*} \\mathbf{x}-\\boldsymbol{\\mu} &= [\\mathbf{x}_{1}-\\boldsymbol{\\mu}_{1}, \\mathbf{x}_{2}-\\boldsymbol{\\mu}_{2}, ..., \\mathbf{x}_{N}-\\boldsymbol{\\mu}_{N}]^T \\\\ \\mathbf{K} &= \\begin{bmatrix} \\sigma_1^2 & 0 & \\cdots & 0\\\\ 0 & \\sigma_2^2 & \\cdots & 0\\\\ \\vdots & \\vdots & \\ddots & 0\\\\ 0 & 0 & 0 & \\sigma_N^2 \\end{bmatrix} \\end{align*}We have \\begin{align*} |\\mathbf{K}|^{\\frac{1}{2}} &= \\sigma_1\\sigma_2...\\sigma_N \\\\ (\\mathbf{x}-\\boldsymbol{\\mu})^T\\mathbf{K}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) &= \\frac{(\\mathbf{x}_{1}-\\boldsymbol{\\mu}_{1})^2}{\\sigma_1^2} + \\frac{(\\mathbf{x}_{2}-\\boldsymbol{\\mu}_{2})^2}{\\sigma_2^2} + ... + \\frac{(\\mathbf{x}_{N}-\\boldsymbol{\\mu}_{N})^2}{\\sigma_N^2} \\end{align*}Thus \\begin{align*} p(\\mathbf{x}) = (2\\pi)^{-\\frac{N}{2}}|\\mathbf{K}|^{-\\frac{1}{2}}\\exp \\left( -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\mathbf{K}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) \\right) \\tag{3}\\label{eq3} \\end{align*}$\\boldsymbol{\\mu} \\in \\mathbb{R}^N$ is the mean vector, $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ is the covariance matrix, since we assume that the dimensions are independent of each other, $\\mathbf{K}$ is a diagonal matrix. When the variables are correlated, the form of Equation $(3)$ is still the same, the covariance matrix $\\mathbf{K}$ is no longer a diagonal matrix and only has the properties of positive semi-definite and symmetric. Equation $(3)$ is usually abbreviated as: \\begin{align*} \\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{K}) \\end{align*}Gaussian ProcessesA Gaussian process is a random process where any point $\\mathbf{x} = [\\mathbf{x}{1}, \\mathbf{x}{2}, \\cdots, \\mathbf{x}{N}]$ is assigned a random variable $f(\\mathbf{x}) = [f(\\mathbf{x}{1}), f(\\mathbf{x}{2}), \\cdots, f(\\mathbf{x}{N})]$ and where the joint distribution of a finite number of these variables $p(f(\\mathbf{x}_1),…,f(\\mathbf{x}_N))$ is itself Gaussian: p(\\mathbf{f} \\lvert \\mathbf{X}) = \\mathcal{N}(\\mathbf{f} \\lvert \\boldsymbol\\mu, \\mathbf{K})\\tag{4}\\label{eq4}In Equation $(4)$, \\mathbf{f} = (f(\\mathbf{x}_1),...,f(\\mathbf{x}_N)), \\boldsymbol\\mu = (m(\\mathbf{x}_1),...,m(\\mathbf{x}_N)) and K_{ij} = \\kappa(\\mathbf{x}_i,\\mathbf{x}_j). $m$ is the mean function and it is common to use $m(\\mathbf{x}) = 0$ as GPs are flexible enough to model the mean arbitrarily well. $\\kappa$ is a positive definite kernel function or covariance function. Thus, a Gaussian process is a distribution over functions whose shape (smoothness, …) is defined by $\\mathbf{K}$. If points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are considered to be similar by the kernel the function values at these points, $f(\\mathbf{x}_i)$ and $f(\\mathbf{x}_j)$, can be expected to be similar too. A GP prior $p(\\mathbf{f} \\lvert \\mathbf{X})$ can be converted into a GP posterior $p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})$ after having observed some data $\\mathbf{y}$. The posterior can then be used to make predictions \\mathbf{f}_* given new input \\mathbf{X}_*: \\begin{align*} p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{X},\\mathbf{y}) &= \\int{p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{f})p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})}\\ d\\mathbf{f} \\\\ &= \\mathcal{N}(\\mathbf{f}_* \\lvert \\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)\\tag{5}\\label{eq5} \\end{align*}Equation $(5)$ is the posterior predictive distribution which is also a Gaussian with mean \\boldsymbol{\\mu}_* and \\boldsymbol{\\Sigma}_*. By definition of the GP, the joint distribution of observed data $\\mathbf{y}$ and predictions \\mathbf{f}_* is \\begin{bmatrix}\\mathbf{y} \\\\ \\mathbf{f}_*\\end{bmatrix} \\sim \\mathcal{N} \\left(\\boldsymbol{0}, \\begin{bmatrix}\\mathbf{K}_y & \\mathbf{K}_* \\\\ \\mathbf{K}_*^T & \\mathbf{K}_{**}\\end{bmatrix} \\right)\\tag{6}\\label{eq6}With $N$ training data and N_* new input data, \\mathbf{K}_y = \\kappa(\\mathbf{X},\\mathbf{X}) + \\sigma_y^2\\mathbf{I} = \\mathbf{K} + \\sigma_y^2\\mathbf{I} is N \\times N, \\mathbf{K}_* = \\kappa(\\mathbf{X},\\mathbf{X}_*) is N \\times N_* and \\mathbf{K}_{**} = \\kappa(\\mathbf{X}_*,\\mathbf{X}_*) is N_* \\times N_*. $\\sigmay^2$ is the noise term in the diagonal of $\\mathbf{K_y}$. It is set to zero if training targets are noise-free and to a value greater than zero if observations are noisy. The mean is set to $\\boldsymbol{0}$ for notational simplicity. The sufficient statistics of the posterior predictive distribution, $$\\boldsymbol{\\mu}and\\boldsymbol{\\Sigma}_$$, can be computed with[1][3] \\begin{align*} \\boldsymbol{\\mu_*} &= \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{y}\\tag{7}\\label{eq7} \\\\ \\boldsymbol{\\Sigma_*} &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{K}_*\\tag{8}\\label{eq8} \\end{align*}This is the minimum we need to know for implementing Gaussian processes and applying them to regression problems. For further details, please consult the literature in the References section. The next section shows how to implement GPs with plain NumPy from scratch, later sections demonstrate how to use GP implementations from scikit-learn. Implementation with NumPyHere, we will use the squared exponential kernel, also known as Gaussian kernel or RBF kernel: \\kappa(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma_f^2 \\exp(-\\frac{1}{2l^2} (\\mathbf{x}_i - \\mathbf{x}_j)^T (\\mathbf{x}_i - \\mathbf{x}_j))\\tag{9}The length parameter $l$ controls the smoothness of the function and $\\sigma_f$ the vertical variation. For simplicity, we use the same length parameter $l$ for all input dimensions (isotropic kernel). 12345678910111213141516import numpy as npdef kernel(X1, X2, l=1.0, sigma_f=1.0): &#x27;&#x27;&#x27; Isotropic squared exponential kernel. Computes a covariance matrix from points in X1 and X2. Args: X1: Array of m points (m x d). X2: Array of n points (n x d). Returns: Covariance matrix (m x n). &#x27;&#x27;&#x27; sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T) return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist) There are many other kernels that can be used for Gaussian processes. See [3] for a detailed reference or the scikit-learn documentation for some examples. PriorLet’s first define a prior over functions with mean zero and a covariance matrix computed with kernel parameters $l=1$ and $\\sigma_f=1$. To draw random functions from that GP we draw random samples from the corresponding multivariate normal. The following example draws ten random samples and plots it together with the zero mean and the 95% confidence interval (computed from the diagonal of the covariance matrix). 1234567891011121314151617181920import matplotlib.pyplot as plt# Finite number of pointsn = 100# points we&#x27;re going to make predictions atX = np.linspace(-5, 5, n).reshape(-1, 1)# Mean and covariance of the priormu_prior = np.zeros(X.shape)cov_prior = kernel(X, X)# Draw 10 samples from the priornumber_of_samples = 10samples_prior = np.random.multivariate_normal(mu_prior.ravel(), cov_prior, number_of_samples)plot_gp(mu_prior, cov_prior, X, samples=samples_prior)plt.title(f&#x27;&#123;number_of_samples&#125; samples from the GP prior (n = 100)&#x27;)plt.axis([-5, 5, -3, 3])plt.legend()plt.show() The plot_gp function is defined here. 12345678def plot_gp(mu, cov, X, X_train=None, Y_train=None, samples=[]): plot_margin_of_error(X, mu, cov) plt.plot(X, mu, label=&#x27;GP mean&#x27;) for i, sample in enumerate(samples): plt.plot(X, sample, lw=1, ls=&#x27;--&#x27;) if X_train is not None: plt.plot(X_train, Y_train, &#x27;rx&#x27;, label=&#x27;Observed Data&#x27;) The plot_margin_of_error function is defined here. 123456def plot_margin_of_error(X, mu, cov): X = X.ravel() mu = mu.ravel() uncertainty = 1.96 * np.sqrt(np.diag(cov)) # %95 Confidence interval plt.fill_between(X, mu + uncertainty, mu - uncertainty, alpha=0.1, label=&#x27;Margin of error (%95 Confidence interval)&#x27;) Prediction from noise-free training dataTo compute the sufficient statistics i.e. mean and covariance of the posterior predictive distribution we implement Equations $(7)$ and $(8)$ 123456789101112131415161718192021222324252627282930from numpy.linalg import invdef posterior_predictive(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8): &#x27;&#x27;&#x27; Computes the sufficient statistics of the GP posterior predictive distribution from m training data X_train and Y_train and n new inputs X_s. Args: X_s: New input locations (n x d). X_train: Training locations (m x d). Y_train: Training targets (m x 1). l: Kernel length parameter. sigma_f: Kernel vertical variation parameter. sigma_y: Noise parameter. Returns: Posterior mean vector (n x d) and covariance matrix (n x n). &#x27;&#x27;&#x27; K = kernel(X_train, X_train, l, sigma_f) + sigma_y ** 2 * np.eye(len(X_train)) K_s = kernel(X_train, X_s, l, sigma_f) K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s)) K_inv = inv(K) # Equation (7) mu_s = K_s.T.dot(K_inv).dot(Y_train) # Equation (8) cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s) return mu_s, cov_s and apply them to noise-free training data X_train and Y_train. The following example draws ten samples from the posterior predictive and plots them along with the mean, confidence interval and training data. In a noise-free model, variance at the training points is zero and all random functions drawn from the posterior go through the trainig points. 123456789101112131415# Noise free training dataX_train = np.array([-3, -2, -1, 1, 2, 3]).reshape(-1, 1)Y_train_noise_free = np.sin(X_train)# Compute mean and covariance of the posterior predictive distributionmu_s, cov_s = posterior_predictive(X, X_train, Y_train_noise_free)# Draw 10 samples from the posteriorsamples_posterior = np.random.multivariate_normal(mu_s.ravel(), cov_s, number_of_samples)plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train_noise_free, samples=samples_posterior)plt.title(f&#x27;&#123;number_of_samples&#125; samples from the GP posterior (n = 100)&#x27;)plt.axis([-5, 5, -3, 3])plt.legend()plt.show() Comparison of true function and GP posterior.12345678910111213141516f_true = np.sin(X).flatten()# Plot Gp meanplt.plot(X, mu_s, ls=&#x27;-&#x27;, label=&#x27;GP mean&#x27;)# Plot data observedplt.plot(X_train, Y_train_noise_free, &#x27;rx&#x27;, label=&#x27;Observed Data&#x27;)# Plot True functionplt.plot(X, f_true, label=&#x27;True function&#x27;)# Plot Margin of error (%95 Confidence interval)plot_margin_of_error(X, mu_s, cov_s)plt.axis([-5, 5, -3, 3])plt.legend()plt.show() Prediction from noisy training dataIf some noise is included in the model, training points are only approximated and the variance at the training points is non-zero. 123456789101112131415# Noisy training datanoise = 0.4X_train = np.array([-3, -2, -1, 1, 2, 3]).reshape(-1, 1)Y_train_noisy = np.sin(X_train) + noise * np.random.randn(*X_train.shape)# Compute mean and covariance of the posterior predictive distributionmu_s, cov_s = posterior_predictive(X, X_train, Y_train_noisy, sigma_y=noise)# Draw 10 samples from the posteriorsamples_posterior_noisy = np.random.multivariate_normal(mu_s.ravel(), cov_s, number_of_samples)plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train_noisy, samples=samples_posterior_noisy)plt.title(f&#x27;&#123;number_of_samples&#125; samples from the GP posterior (n = 100)&#x27;)plt.axis([-5, 5, -3, 3])plt.legend()plt.show() Comparison of true function and GP posterior.12345678910111213141516f_true = np.sin(X).flatten()# Plot Gp meanplt.plot(X, mu_s, ls=&#x27;-&#x27;, label=&#x27;GP mean&#x27;)# Plot data observedplt.plot(X_train, Y_train_noisy, &#x27;rx&#x27;, label=&#x27;Observed Data&#x27;)# Plot True functionplt.plot(X, f_true, label=&#x27;True function&#x27;)# Plot Margin of error (%95 Confidence interval)plot_margin_of_error(X, mu_s, cov_s)plt.axis([-5, 5, -3, 3])plt.legend()plt.show() Effect of kernel parameters and noise parameterThe following example shows the effect of kernel parameters $l$ and $\\sigma_f$ as well as the noise parameter $\\sigma_y$. Higher $l$ values lead to smoother functions and therefore to coarser approximations of the training data. Lower $l$ values make functions more wiggly with wide confidence intervals between training data points. $\\sigma_f$ controls the vertical variation of functions drawn from the GP. This can be seen by the wide confidence intervals outside the training data region in the right figure of the second row. $\\sigma_y$ represents the amount of noise in the training data. Higher $\\sigma_y$ values make more coarse approximations which avoids overfitting to noisy data. 1234567891011121314151617params = [ (0.3, 1.0, 0.2), (3.0, 1.0, 0.2), (1.0, 0.3, 0.2), (1.0, 3.0, 0.2), (1.0, 1.0, 0.05), (1.0, 1.0, 1.5),]for i, (l, sigma_f, sigma_y) in enumerate(params): mu_s, cov_s = posterior_predictive(X, X_train, Y_train_noisy, l=l, sigma_f=sigma_f, sigma_y=sigma_y) plt.title(f&#x27;l = &#123;l&#125;, sigma_f = &#123;sigma_f&#125;, sigma_y = &#123;sigma_y&#125;&#x27;) plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train_noisy) plt.axis([-5, 5, -3, 3]) plt.show() Optimal values for these parameters can be estimated by maximizing the log marginal likelihood which is given by[1][3] \\begin{align*} \\log p(\\mathbf{y} \\lvert \\mathbf{X}) &=\\log \\mathcal{N}(\\mathbf{y} \\lvert \\boldsymbol{0},\\mathbf{K}_y) \\\\ &=-\\frac{1}{2} \\mathbf{y}^T \\mathbf{K}_y^{-1} \\mathbf{y} -\\frac{1}{2} \\log \\begin{vmatrix}\\mathbf{K}_y\\end{vmatrix} -\\frac{N}{2} \\log(2\\pi) \\tag{10} \\end{align*}In the following we will minimize the negative log marginal likelihood w.r.t. parameters $l$ and $\\sigma_f$, $\\sigma_y$ is set to the known noise level of the data. If the noise level is unknown, $\\sigma_y$ can be estimated as well along with the other parameters. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667from numpy.linalg import cholesky, det, lstsqfrom scipy.optimize import minimizedef nll_fn(X_train, Y_train, noise, naive=True): &#x27;&#x27;&#x27; Returns a function that computes the negative log marginal likelihood for training data X_train and Y_train and given noise level. Args: X_train: training locations (m x d). Y_train: training targets (m x 1). noise: known noise level of Y_train. naive: if True use a naive implementation of Eq. (10), if False use a numerically more stable implementation. Returns: Minimization objective. &#x27;&#x27;&#x27; def nll_naive(theta): # Naive implementation of Eq. (10). Works well for the examples # in this article but is numerically less stable compared to # the implementation in nll_stable below. K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\ noise ** 2 * np.eye(len(X_train)) return 0.5 * np.log(det(K)) + \\ 0.5 * Y_train.T.dot(inv(K).dot(Y_train)) + \\ 0.5 * len(X_train) * np.log(2 * np.pi) def nll_stable(theta): # Numerically more stable implementation of Eq. (10) as described # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section # 2.2, Algorithm 2.1. K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\ noise ** 2 * np.eye(len(X_train)) L = cholesky(K) return np.sum(np.log(np.diagonal(L))) + \\ 0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\ 0.5 * len(X_train) * np.log(2 * np.pi) if naive: return nll_naive else: return nll_stable# Minimize the negative log-likelihood w.r.t. parameters l and sigma_f.# We should actually run the minimization several times with different# initializations to avoid local minimal but this is skipped here for# simplicity.res = minimize(nll_fn(X_train, Y_train_noisy, noise), [1, 1], bounds=((1e-5, None), (1e-5, None)), method=&#x27;L-BFGS-B&#x27;)# Store the optimization results in global variables so that we can# compare it later with the results from other implementations.l_opt, sigma_f_opt = res.xl_opt, sigma_f_opt# Compute the posterior predictive statistics with optimized kernel parameters and plot the resultsmu_s, cov_s = posterior_predictive(X, X_train, Y_train_noisy, l=l_opt, sigma_f=sigma_f_opt, sigma_y=noise)plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train_noisy)plt.axis([-5, 5, -3, 3])plt.title(f&#x27;After parameter optimization:l = &#123;l_opt:.2f&#125;, sigma_f = &#123;sigma_f_opt:.2f&#125;, sigma_y = &#123;noise&#125;&#x27;)plt.legend()plt.show() With optimized kernel parameters, training data are reasonably covered by the 95% confidence interval and the mean of the posterior predictive is a good approximation. Higher dimensionsThe above implementation can also be used for higher input data dimensions. Here, a GP is used to fit noisy samples from a sine wave originating at $\\boldsymbol{0}$ and expanding in the x-y plane. The following plots show the noisy samples and the posterior predictive mean before and after kernel parameter optimization. 123456789101112131415161718192021222324252627282930313233343536from matplotlib import cmfrom mpl_toolkits.mplot3d import Axes3Ddef plot_gp_2D(gx, gy, mu, X_train, Y_train, title, i): ax = plt.gcf().add_subplot(1, 2, i, projection=&#x27;3d&#x27;) ax.plot_surface(gx, gy, mu.reshape(gx.shape), cmap=cm.coolwarm, linewidth=0, alpha=0.2, antialiased=False) ax.scatter(X_train[:, 0], X_train[:, 1], Y_train, c=Y_train, cmap=cm.coolwarm) z = mu.reshape(gx.shape) ax.contourf(gx, gy, z, zdir=&#x27;z&#x27;, offset=0, cmap=cm.coolwarm, alpha=0.6) ax.set_title(title)noise_2D = 0.1rx, ry = np.arange(-5, 5, 0.3), np.arange(-5, 5, 0.3)gx, gy = np.meshgrid(rx, rx)X_2D = np.c_[gx.ravel(), gy.ravel()]X_2D_train = np.random.uniform(-4, 4, (100, 2))Y_2D_train = np.sin(0.5 * np.linalg.norm(X_2D_train, axis=1)) + \\ noise_2D * np.random.randn(len(X_2D_train))plt.figure(figsize=(14, 7))mu_s, _ = posterior_predictive(X_2D, X_2D_train, Y_2D_train, sigma_y=noise_2D)plot_gp_2D(gx, gy, mu_s, X_2D_train, Y_2D_train, f&#x27;Before parameter optimization: l=&#123;1.00&#125; sigma_f=&#123;1.00&#125;&#x27;, 1)res = minimize(nll_fn(X_2D_train, Y_2D_train, noise_2D), [1, 1], bounds=((1e-5, None), (1e-5, None)), method=&#x27;L-BFGS-B&#x27;)mu_s, _ = posterior_predictive(X_2D, X_2D_train, Y_2D_train, *res.x, sigma_y=noise_2D)plot_gp_2D(gx, gy, mu_s, X_2D_train, Y_2D_train, f&#x27;After parameter optimization: l=&#123;res.x[0]:.2f&#125; sigma_f=&#123;res.x[1]:.2f&#125;&#x27;, 2)plt.show() Note how the true sine wave is approximated much better after parameter optimization. Library that implement GPsThis section shows a example of library that provide implementation of GPs. Only a minimal setup will be provided here, just enough for reproducing the above results. For further details please consult the documentation of the library. Scikit-learnScikit-learn provides a GaussianProcessRegressor for implementing GP regression models. It can be configured with pre-defined kernels and user-defined kernels. Kernels can also be composed. The squared exponential kernel is the RBF kernel in scikit-learn. The RBF kernel only has a length_scale parameter which corresponds to the $l$ parameter above. To have a $\\sigma_f$ parameter as well, we have to compose the RBF kernel with a ConstantKernel. 12345678910111213141516171819202122from sklearn.gaussian_process import GaussianProcessRegressorfrom sklearn.gaussian_process.kernels import ConstantKernel, RBFrbf = ConstantKernel(1.0) * RBF(length_scale=1.0)gpr = GaussianProcessRegressor(kernel=rbf, alpha=noise**2)# Reuse training data from previous 1D examplegpr.fit(X_train, Y_train)# Compute posterior predictive mean and covariancemu_s, cov_s = gpr.predict(X, return_cov=True)# Obtain optimized kernel parametersl = gpr.kernel_.k2.get_params()[&#x27;length_scale&#x27;]sigma_f = np.sqrt(gpr.kernel_.k1.get_params()[&#x27;constant_value&#x27;])# Compare with previous resultsassert(np.isclose(l_opt, l))assert(np.isclose(sigma_f_opt, sigma_f))# Plot the resultsplot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train) References[1] Kevin P. Murphy. Machine Learning, A Probabilistic Perspective, Chapters 4, 14 and 15.[2] Christopher M. Bishop. Pattern Recognition and Machine Learning, Chapter 6.[3] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.[4] Guibo Wang. Gaussian processes.[5] Martin Krasser. Gaussian processes.","categories":[],"tags":[]}],"categories":[{"name":"Tutorial","slug":"Tutorial","permalink":"https://siqindong.com/categories/Tutorial/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://siqindong.com/categories/Algorithm/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://siqindong.com/tags/ROS/"},{"name":"Matlab","slug":"Matlab","permalink":"https://siqindong.com/tags/Matlab/"}]}